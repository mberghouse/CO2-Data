{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything checks out except for the 0s that should be nan. I think these 0s come when a site does not have a variable (happens alot with water potential and reduction potential), so the code seems to just add a column of 0s for those sites\n",
    "\n",
    "# Also, if a site has a variable (such as soil moisture) at one depth but CO2 at 2 depths, then we will get 0s instead of nan for the depth where we have CO2 but not soil moisture.\n",
    "\n",
    "## Some 0s should be nan for Calhoun\n",
    "## BGZOB water potential and reduction potential should be nan at the beginning, not 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# sites is a list of locations (e.g., \"Calhoun\"),\n",
    "# all_pits is a dict with sites as keys, and lists of pits \n",
    "# at each site as the values\n",
    "all_pits = {}\n",
    "sites = os.listdir('../../processed_data')\n",
    "\n",
    "# Loop through all sites to construct dict of sites/pits\n",
    "for site in sites:\n",
    "    cur_pits = []\n",
    "    for pro_file in os.listdir(os.path.join('../../processed_data/', site)):\n",
    "        cur_pits.append(pro_file.split('_')[0])\n",
    "        \n",
    "    all_pits[site] = cur_pits\n",
    "\n",
    "# Data will be a dict with pits as keys, np arrays as values\n",
    "data = {}\n",
    "# This is a list of all the features for R1C1. We'll want to make this an exhaustive list\n",
    "# of all potential features across all pits\n",
    "features = ['CO2', 'precip', 'SoilMoisture', 'BulkEC', 'Temp', 'O2', 'WaterPotential', 'ReductionPotential']\n",
    "m = len(features)\n",
    "\n",
    "# Load in and merge all files\n",
    "# NOTE: Only doing Calhoun R1C1 for now, but you get the idea\n",
    "i = 0\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        \n",
    "        if pit == 'R1C1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'R1H1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "    \n",
    "        if pit == 'R1P1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'LRMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'TMMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'NPMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'SPVF':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "          \n",
    "      \n",
    "        if pit == 'BGZOB1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "        if pit == 'BGZOB3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'Green2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC5':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC6':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1C1\n",
      "R1C2\n",
      "R1H1\n",
      "R1P1\n",
      "BGZOB1\n",
      "BGZOB2\n",
      "BGZOB3\n",
      "BGZOB4\n",
      "Green1\n",
      "Green2\n",
      "Green3\n",
      "MC1\n",
      "MC2\n",
      "MC3\n",
      "MC4\n",
      "MC5\n",
      "MC6\n",
      "SFPit1\n",
      "LRMS\n",
      "NPMS\n",
      "SPMS\n",
      "SPVF\n",
      "TMMS\n",
      "['30']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        print (pit)\n",
    "print (unique_depths)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1C1': array([[1.31800000e+04,            nan, 3.58000000e-01, ...,\n",
       "         1.94093357e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.35700000e+04, 0.00000000e+00, 3.58000000e-01, ...,\n",
       "         1.94130193e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.38300000e+04, 0.00000000e+00, 3.58000000e-01, ...,\n",
       "         1.94173727e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00],\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00],\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'R1H1': array([[1.31500000e+04,            nan, 3.16000000e-01, ...,\n",
       "         2.01908655e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.57350000e+04,            nan, 3.16000000e-01, ...,\n",
       "         1.99086872e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.81150000e+04,            nan, 3.17000000e-01, ...,\n",
       "         1.94680690e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [3.39250000e+04,            nan, 0.00000000e+00, ...,\n",
       "         1.60400000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [3.39200000e+04,            nan, 0.00000000e+00, ...,\n",
       "         1.60400000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [3.39500000e+04,            nan, 0.00000000e+00, ...,\n",
       "         1.60400000e+01, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'R1P1': array([[6.11000000e+03,            nan, 3.49000000e-01, ...,\n",
       "         2.05010845e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [6.28000000e+03,            nan, 3.49000000e-01, ...,\n",
       "         2.04233205e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [6.37500000e+03,            nan, 3.49000000e-01, ...,\n",
       "         2.04197855e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00],\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00],\n",
       "        [           nan,            nan, 0.00000000e+00, ...,\n",
       "                    nan, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'BGZOB1': array([[3.80855200e+03,            nan,            nan, ...,\n",
       "         2.07486667e+01,            nan,            nan],\n",
       "        [4.22788325e+03,            nan,            nan, ...,\n",
       "         2.09152500e+01,            nan,            nan],\n",
       "        [4.40697625e+03,            nan,            nan, ...,\n",
       "         2.08955000e+01,            nan,            nan],\n",
       "        ...,\n",
       "        [1.76544625e+03,            nan, 8.40000000e-02, ...,\n",
       "         1.93565000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.78305375e+03,            nan, 8.40000000e-02, ...,\n",
       "         1.93572500e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.78446775e+03,            nan, 8.40000000e-02, ...,\n",
       "         1.93570000e+01, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'BGZOB2': array([[1.04866550e+04,            nan,            nan, ...,\n",
       "         1.93290000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.04092450e+04,            nan,            nan, ...,\n",
       "         1.96255000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.04389000e+04,            nan,            nan, ...,\n",
       "         1.96210000e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.72979275e+03,            nan, 9.92500000e-02, ...,\n",
       "         1.95177500e+01, 0.00000000e+00, 6.40675000e+02],\n",
       "        [2.73814325e+03,            nan, 9.90000000e-02, ...,\n",
       "         1.95232500e+01, 0.00000000e+00, 6.40850000e+02],\n",
       "        [2.73374950e+03,            nan, 9.90000000e-02, ...,\n",
       "         1.95382500e+01, 0.00000000e+00, 6.41000000e+02]]),\n",
       " 'BGZOB3': array([[1.80614225e+03,            nan,            nan, ...,\n",
       "         1.97577500e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.84162050e+03,            nan,            nan, ...,\n",
       "         2.14672500e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.84134750e+03,            nan,            nan, ...,\n",
       "         2.14307500e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [7.25851350e+03,            nan, 1.07000000e-01, ...,\n",
       "         2.03260000e+01, 0.00000000e+00, 5.11900000e+02],\n",
       "        [7.28852250e+03,            nan, 1.07000000e-01, ...,\n",
       "         2.03250000e+01, 0.00000000e+00, 5.11900000e+02],\n",
       "        [7.31488850e+03,            nan, 1.07000000e-01, ...,\n",
       "         2.03265000e+01, 0.00000000e+00, 5.11900000e+02]]),\n",
       " 'BGZOB4': array([[ 2.32920625e+03,             nan,             nan, ...,\n",
       "          1.98972500e+01,             nan,             nan],\n",
       "        [ 2.48302675e+03,             nan,             nan, ...,\n",
       "          1.99022500e+01,             nan,             nan],\n",
       "        [ 2.57689150e+03,             nan,  1.13750000e-01, ...,\n",
       "          1.98712500e+01, -1.32000000e+01,             nan],\n",
       "        ...,\n",
       "        [ 2.34175700e+03,             nan,  1.33000000e-01, ...,\n",
       "          1.99600000e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.35045450e+03,             nan,  1.33000000e-01, ...,\n",
       "          1.99652500e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.34931625e+03,             nan,  1.33000000e-01, ...,\n",
       "          1.99815000e+01,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'Green1': array([[ 1.55446500e+03,             nan,  1.71000000e-01, ...,\n",
       "          2.10060000e+01, -1.29000000e+01,  0.00000000e+00],\n",
       "        [ 1.57059525e+03,             nan,  1.71000000e-01, ...,\n",
       "          2.12122500e+01, -1.33750000e+01,  0.00000000e+00],\n",
       "        [ 1.56044225e+03,             nan,  1.71000000e-01, ...,\n",
       "          2.11922500e+01, -1.35500000e+01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.45685900e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93095000e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.48362400e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93047500e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.48786000e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93042500e+01,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'Green2': array([[ 1.16653445e+03,             nan,  4.44545455e-02, ...,\n",
       "          2.11869091e+01, -1.41818182e+01,  0.00000000e+00],\n",
       "        [ 2.59860250e+03,             nan,  4.51250000e-02, ...,\n",
       "          2.10649375e+01, -1.46187500e+01,  0.00000000e+00],\n",
       "        [ 2.70756400e+03,             nan,  4.63750000e-02, ...,\n",
       "          2.11018125e+01, -1.36750000e+01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 1.61920100e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.96787500e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.53076350e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.97151875e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.63845825e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.96901250e+01,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'Green3': array([[ 2.48702100e+03,             nan,  3.80000000e-02, ...,\n",
       "          2.03500000e+01, -1.00000000e+01,  0.00000000e+00],\n",
       "        [ 2.50080475e+03,             nan,  3.80000000e-02, ...,\n",
       "          2.07245000e+01, -1.00250000e+01,  0.00000000e+00],\n",
       "        [ 2.53194150e+03,             nan,  3.87500000e-02, ...,\n",
       "          2.07010000e+01, -1.00500000e+01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.30625600e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93630000e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.33335775e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93602500e+01,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 2.33797975e+03,             nan,  0.00000000e+00, ...,\n",
       "          1.93585000e+01,  0.00000000e+00,  0.00000000e+00]]),\n",
       " 'MC1': array([[nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC2': array([[ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC3': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC4': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC5': array([[    nan,     nan,     nan, ...,     nan,   0.   ,     nan],\n",
       "        [    nan,     nan,     nan, ...,     nan,   0.   ,     nan],\n",
       "        [    nan,     nan,     nan, ...,     nan,   0.   ,     nan],\n",
       "        ...,\n",
       "        [    nan,     nan,   0.   , ...,     nan,   0.   , 398.45 ],\n",
       "        [    nan,     nan,   0.   , ...,     nan,   0.   , 398.225],\n",
       "        [    nan,     nan,   0.   , ...,     nan,   0.   , 397.75 ]]),\n",
       " 'MC6': array([[nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'LRMS': array([[ 8.1081095,        nan,  0.       , ...,        nan,  0.       ,\n",
       "          0.       ],\n",
       "        [ 2.702703 ,        nan,  0.       , ..., 18.030001 ,  0.       ,\n",
       "          0.       ],\n",
       "        [ 5.405406 ,        nan,  0.       , ..., 18.51     ,  0.       ,\n",
       "          0.       ],\n",
       "        ...,\n",
       "        [       nan,        nan,  0.       , ...,        nan,  0.       ,\n",
       "          0.       ],\n",
       "        [       nan,        nan,  0.       , ...,        nan,  0.       ,\n",
       "          0.       ],\n",
       "        [       nan,        nan,  0.       , ...,        nan,  0.       ,\n",
       "          0.       ]]),\n",
       " 'NPMS': array([[2985.074   ,         nan,    0.      , ...,   20.690001,\n",
       "            0.      ,    0.      ],\n",
       "        [2957.937   ,         nan,    0.      , ...,   20.610001,\n",
       "            0.      ,    0.      ],\n",
       "        [2887.4031  ,         nan,    0.      , ...,   20.610001,\n",
       "            0.      ,    0.      ],\n",
       "        ...,\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ]]),\n",
       " 'SPVF': array([[8.30000000e+002,             nan, 1.30490959e-311, ...,\n",
       "         2.13831246e+001, 1.30490959e-311, 1.30490956e-311],\n",
       "        [8.44500000e+002,             nan, 1.30490960e-311, ...,\n",
       "         2.13474162e+001, 1.30490960e-311, 1.30490959e-311],\n",
       "        [8.58500000e+002,             nan, 1.30490960e-311, ...,\n",
       "         2.13250665e+001, 1.30490960e-311, 1.30490961e-311],\n",
       "        ...,\n",
       "        [            nan,             nan, 0.00000000e+000, ...,\n",
       "                     nan, 0.00000000e+000, 0.00000000e+000],\n",
       "        [            nan,             nan, 0.00000000e+000, ...,\n",
       "                     nan, 0.00000000e+000, 0.00000000e+000],\n",
       "        [            nan,             nan, 0.00000000e+000, ...,\n",
       "                     nan, 0.00000000e+000, 0.00000000e+000]]),\n",
       " 'TMMS': array([[5043.2441  ,         nan,    0.      , ...,   20.110001,\n",
       "            0.      ,    0.      ],\n",
       "        [5113.5142  ,         nan,    0.      , ...,   20.09    ,\n",
       "            0.      ,    0.      ],\n",
       "        [4994.5952  ,         nan,    0.      , ...,   20.059999,\n",
       "            0.      ,    0.      ],\n",
       "        ...,\n",
       "        [4066.6121  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ],\n",
       "        [4009.4121  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ],\n",
       "        [3888.4971  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1=pd.DataFrame(data, index=['A'])\n",
    "data\n",
    "#cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=np.vstack((data['R1C1'], data['R1H1'], data['R1P1'], data['BGZOB1'], data['BGZOB2'], data['BGZOB3'], data['BGZOB4'], data['Green1'], data['Green2'], data['Green3'], data['MC1'], data['MC2'], data['MC3'], data['MC4'], data['MC5'], data['MC6'], data['TMMS'], data['LRMS'], data['NPMS'], data['SPVF']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we'll need to make sure all the data is in the same units, across all sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the goal is to build up a numpy array where each row is an observation (an individual site/depth/time; e.g., Calhoun R1C1 on January 1st at 12:15 pm) and each column is a parameter (e.g., CO2, Soil moisture, O2, etc). We'll have lots of NaN values, and that's fine for now.\n",
    "\n",
    "What I would recommend doing is building up one site/pit at a time, then combining arrays later on. For example, the R1C1 array should look like:\n",
    "\n",
    "|     |CO2|precip|SoilMoisture|BulkEC|Temp|O2|\n",
    "|---|---|---|---|---|---|---|\n",
    "|12/12/20 12:15 pm @ 25 cm | 5000 ppm | 0 | 0.24 | NaN | 4.37 | 19.1 |\n",
    "|12/12/20 12:30 pm @ 25 cm | 5169 ppm | 0 | 0.26 | NaN | 4.45 | 19.2 |\n",
    "|12/12/20 12:45 pm @ 25 cm | 5120 ppm | 0 | 0.29 | NaN | 4.42 | 19.1 |\n",
    "|12/12/20 01:00 pm @ 25 cm | 5148 ppm | 0 | 0.26 | NaN | 4.49 | 19.2 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "|07/04/18 09:15 am @ 150 cm | 6952 ppm | 0.01 | 0.39 | NaN | 4.3 | 19.1 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| etc | etc | etc | etc | etc | etc | etc |\n",
    "\n",
    "Except without the columns or index labels. You could also set it up as a pandas dataframe (ie, with column and index labels) then extract the values later on. Whichever is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=merged.copy()\n",
    "outfile = '../../merged_processed_data/MergeProcessedData.csv'\n",
    "\n",
    "np.savetxt(outfile, output, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.31800000e+04,            nan, 3.58000000e-01, ...,\n",
       "        1.94093357e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.35700000e+04, 0.00000000e+00, 3.58000000e-01, ...,\n",
       "        1.94130193e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.38300000e+04, 0.00000000e+00, 3.58000000e-01, ...,\n",
       "        1.94173727e+01, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [           nan,            nan, 0.00000000e+00, ...,\n",
       "                   nan, 0.00000000e+00, 0.00000000e+00],\n",
       "       [           nan,            nan, 0.00000000e+00, ...,\n",
       "                   nan, 0.00000000e+00, 0.00000000e+00],\n",
       "       [           nan,            nan, 0.00000000e+00, ...,\n",
       "                   nan, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
