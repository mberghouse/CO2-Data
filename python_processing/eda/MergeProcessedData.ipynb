{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything checks out except for the 0s that should be nan. I think these 0s come when a site does not have a variable (happens alot with water potential and reduction potential), so the code seems to just add a column of 0s for those sites\n",
    "\n",
    "# Also, if a site has a variable (such as soil moisture) at one depth but CO2 at 2 depths, then we will get 0s instead of nan for the depth where we have CO2 but not soil moisture.\n",
    "\n",
    "## Some 0s should be nan for Calhoun\n",
    "## BGZOB water potential and reduction potential should be nan at the beginning, not 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# sites is a list of locations (e.g., \"Calhoun\"),\n",
    "# all_pits is a dict with sites as keys, and lists of pits \n",
    "# at each site as the values\n",
    "all_pits = {}\n",
    "sites = os.listdir('../../processed_data')\n",
    "\n",
    "# Loop through all sites to construct dict of sites/pits\n",
    "for site in sites:\n",
    "    cur_pits = []\n",
    "    for pro_file in os.listdir(os.path.join('../../processed_data/', site)):\n",
    "        cur_pits.append(pro_file.split('_')[0])\n",
    "        \n",
    "    all_pits[site] = cur_pits\n",
    "\n",
    "# Data will be a dict with pits as keys, np arrays as values\n",
    "data = {}\n",
    "# This is a list of all the features for R1C1. We'll want to make this an exhaustive list\n",
    "# of all potential features across all pits\n",
    "features = ['CO2', 'precip', 'SoilMoisture', 'BulkEC', 'Temp', 'O2', 'WaterPotential', 'ReductionPotential']\n",
    "m = len(features)\n",
    "\n",
    "# Load in and merge all files\n",
    "# NOTE: Only doing Calhoun R1C1 for now, but you get the idea\n",
    "i = 0\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        \n",
    "        if pit == 'R1C1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'R1H1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "    \n",
    "        if pit == 'R1P1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'LRMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,7]=np.nan\n",
    "                    cur_data[:,6]=np.nan\n",
    "                    cur_data[:,3]=np.nan\n",
    "                    cur_data[:,2]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'TMMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,7]=np.nan\n",
    "                    cur_data[:,6]=np.nan\n",
    "                    cur_data[:,3]=np.nan\n",
    "                    cur_data[:,2]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'NPMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,7]=np.nan\n",
    "                    cur_data[:,6]=np.nan\n",
    "                    cur_data[:,3]=np.nan\n",
    "                    cur_data[:,2]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'SPVF':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,7]=np.nan\n",
    "                    cur_data[:,6]=np.nan\n",
    "                    cur_data[:,3]=np.nan\n",
    "                    cur_data[:,2]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "          \n",
    "      \n",
    "        if pit == 'BGZOB1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "                        if np.all(df[col])==0:\n",
    "                            df[col]=np.nan\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "        if pit == 'BGZOB3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'Green2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC5':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC6':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                        if np.all(cur_data[:,j]==0)==True:\n",
    "                            cur_data[:,j]=np.nan       \n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    cur_data[:,6]=np.nan\n",
    "                if np.all(cur_data[:,j]==0)==True:\n",
    "                        cur_data[:,j]=np.nan\n",
    "                    \n",
    "            data[pit] = cur_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if np.all(cur_data[:,7]==0)==True:\n",
    "    print ('True')\n",
    "else:\n",
    "    print ('False')\n",
    "#np.all(cur_data[:,1])==0   \n",
    "np.all(cur_data[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1C1\n",
      "R1C2\n",
      "R1H1\n",
      "R1P1\n",
      "BGZOB1\n",
      "BGZOB2\n",
      "BGZOB3\n",
      "BGZOB4\n",
      "Green1\n",
      "Green2\n",
      "Green3\n",
      "MC1\n",
      "MC2\n",
      "MC3\n",
      "MC4\n",
      "MC5\n",
      "MC6\n",
      "SFPit1\n",
      "LRMS\n",
      "NPMS\n",
      "SPMS\n",
      "SPVF\n",
      "TMMS\n",
      "['30']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O2_30cm.percent.0-100'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        print (pit)\n",
    "print (unique_depths)\n",
    "matches\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1C1': array([[7495.        ,           nan,           nan, ...,   18.98822441,\n",
       "                   nan,           nan],\n",
       "        [7715.        ,    0.        ,           nan, ...,   18.76119806,\n",
       "                   nan,           nan],\n",
       "        [7910.        ,    0.        ,           nan, ...,   18.76119806,\n",
       "                   nan,           nan],\n",
       "        ...,\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan],\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan],\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan]]),\n",
       " 'R1H1': array([[           nan,            nan,            nan, ...,\n",
       "         1.99306457e+01,            nan,            nan],\n",
       "        [           nan,            nan,            nan, ...,\n",
       "         1.97646571e+01,            nan,            nan],\n",
       "        [           nan,            nan,            nan, ...,\n",
       "         1.94311484e+01,            nan,            nan],\n",
       "        ...,\n",
       "        [1.00300000e+04,            nan, 2.10000000e-01, ...,\n",
       "         1.74700000e+01,            nan,            nan],\n",
       "        [1.00250000e+04,            nan, 2.10000000e-01, ...,\n",
       "         1.74700000e+01,            nan,            nan],\n",
       "        [1.01100000e+04,            nan, 2.10000000e-01, ...,\n",
       "         1.74700000e+01,            nan,            nan]]),\n",
       " 'R1P1': array([[        nan,         nan,         nan, ..., 16.17461227,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ..., 16.17236627,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ..., 15.89434463,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]]),\n",
       " 'BGZOB1': array([[ 3.80855200e+03,             nan,             nan, ...,\n",
       "          2.07486667e+01,             nan,             nan],\n",
       "        [ 4.22788325e+03,             nan,             nan, ...,\n",
       "          2.09152500e+01,             nan,             nan],\n",
       "        [ 4.40697625e+03,             nan,             nan, ...,\n",
       "          2.08955000e+01,             nan,             nan],\n",
       "        ...,\n",
       "        [ 1.28422825e+03,             nan,  1.09000000e-01, ...,\n",
       "          1.95505000e+01, -7.99125000e+03,  5.20775000e+02],\n",
       "        [ 1.28530025e+03,             nan,  1.09000000e-01, ...,\n",
       "          1.95555000e+01, -7.99900000e+03,  5.20925000e+02],\n",
       "        [ 1.28664600e+03,             nan,  1.08750000e-01, ...,\n",
       "          1.95605000e+01, -7.99900000e+03,  5.21100000e+02]]),\n",
       " 'BGZOB2': array([[ 3.20011950e+03,             nan,             nan, ...,\n",
       "          1.99910000e+01,             nan,             nan],\n",
       "        [ 3.20603450e+03,             nan,             nan, ...,\n",
       "          2.02915000e+01,             nan,             nan],\n",
       "        [ 3.18802425e+03,             nan,             nan, ...,\n",
       "          2.02560000e+01,             nan,             nan],\n",
       "        ...,\n",
       "        [ 9.41978500e+02,             nan,  1.03000000e-01, ...,\n",
       "          1.74922500e+01, -3.53575000e+02,  5.61025000e+02],\n",
       "        [ 9.40503750e+02,             nan,  1.02250000e-01, ...,\n",
       "          1.75070000e+01, -3.57525000e+02,  5.61400000e+02],\n",
       "        [ 9.37238750e+02,             nan,  1.02000000e-01, ...,\n",
       "          1.75122500e+01, -3.64025000e+02,  5.61925000e+02]]),\n",
       " 'BGZOB3': array([[ 3.04434450e+03,             nan,             nan, ...,\n",
       "          2.06890000e+01,             nan,             nan],\n",
       "        [ 3.10607825e+03,             nan,             nan, ...,\n",
       "          2.06442500e+01,             nan,             nan],\n",
       "        [ 3.09619650e+03,             nan,             nan, ...,\n",
       "          2.06512500e+01,             nan,             nan],\n",
       "        ...,\n",
       "        [ 1.40145675e+03,             nan,  1.07000000e-01, ...,\n",
       "          1.84402500e+01, -5.91750000e+01,  5.21550000e+02],\n",
       "        [ 1.40152500e+03,             nan,  1.07000000e-01, ...,\n",
       "          1.84485000e+01, -6.01500000e+01,  5.22100000e+02],\n",
       "        [ 1.39949850e+03,             nan,  1.07000000e-01, ...,\n",
       "          1.84707500e+01, -6.06250000e+01,  5.23050000e+02]]),\n",
       " 'BGZOB4': array([[ 2.32920625e+03,             nan,             nan, ...,\n",
       "          1.98972500e+01,             nan,             nan],\n",
       "        [ 2.48302675e+03,             nan,             nan, ...,\n",
       "          1.99022500e+01,             nan,             nan],\n",
       "        [ 2.57689150e+03,             nan,  1.13750000e-01, ...,\n",
       "          1.98712500e+01, -1.32000000e+01,             nan],\n",
       "        ...,\n",
       "        [ 6.84631000e+02,             nan,  1.15000000e-01, ...,\n",
       "          1.81785000e+01, -1.17800000e+02,  4.97900000e+02],\n",
       "        [ 6.84471500e+02,             nan,  1.15000000e-01, ...,\n",
       "          1.81862500e+01, -1.17975000e+02,  4.99175000e+02],\n",
       "        [ 6.85564500e+02,             nan,  1.15000000e-01, ...,\n",
       "          1.81890000e+01, -1.19525000e+02,  5.00375000e+02]]),\n",
       " 'Green1': array([[ 8.8436700e+02,            nan,  2.5750000e-01, ...,\n",
       "                    nan, -1.2900000e+01,            nan],\n",
       "        [ 9.2773575e+02,            nan,  2.5775000e-01, ...,\n",
       "                    nan, -1.3550000e+01,            nan],\n",
       "        [ 9.9756500e+02,            nan,  2.5800000e-01, ...,\n",
       "                    nan, -1.3425000e+01,            nan],\n",
       "        ...,\n",
       "        [ 2.4568590e+03,            nan,            nan, ...,\n",
       "          1.9309500e+01,            nan,            nan],\n",
       "        [ 2.4836240e+03,            nan,            nan, ...,\n",
       "          1.9304750e+01,            nan,            nan],\n",
       "        [ 2.4878600e+03,            nan,            nan, ...,\n",
       "          1.9304250e+01,            nan,            nan]]),\n",
       " 'Green2': array([[ 5.58145636e+02,             nan,  1.69818182e-01, ...,\n",
       "          2.08950000e+01, -9.57272727e+00,             nan],\n",
       "        [ 1.67497663e+03,             nan,  1.70625000e-01, ...,\n",
       "          2.11452500e+01, -9.60625000e+00,             nan],\n",
       "        [ 1.87421306e+03,             nan,  1.71000000e-01, ...,\n",
       "          2.11975625e+01, -9.33125000e+00,             nan],\n",
       "        ...,\n",
       "        [ 1.61920100e+03,             nan,             nan, ...,\n",
       "          1.96787500e+01,             nan,             nan],\n",
       "        [ 1.53076350e+03,             nan,             nan, ...,\n",
       "          1.97151875e+01,             nan,             nan],\n",
       "        [ 1.63845825e+03,             nan,             nan, ...,\n",
       "          1.96901250e+01,             nan,             nan]]),\n",
       " 'Green3': array([[ 2.03324100e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.02070000e+01, -1.04000000e+01,             nan],\n",
       "        [ 2.03914300e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.11652500e+01, -1.02750000e+01,             nan],\n",
       "        [ 2.05904400e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.12462500e+01, -1.03000000e+01,             nan],\n",
       "        ...,\n",
       "        [ 2.30625600e+03,             nan,             nan, ...,\n",
       "          1.93630000e+01,             nan,             nan],\n",
       "        [ 2.33335775e+03,             nan,             nan, ...,\n",
       "          1.93602500e+01,             nan,             nan],\n",
       "        [ 2.33797975e+03,             nan,             nan, ...,\n",
       "          1.93585000e+01,             nan,             nan]]),\n",
       " 'MC1': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'MC2': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'MC3': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'MC4': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'MC5': array([[    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "        [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "        ...,\n",
       "        [    nan,     nan,     nan, ...,     nan,     nan, 497.275],\n",
       "        [    nan,     nan,     nan, ...,     nan,     nan, 497.075],\n",
       "        [    nan,     nan,     nan, ...,     nan,     nan, 497.525]]),\n",
       " 'MC6': array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan]]),\n",
       " 'LRMS': array([[ 8.1081095,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [ 2.702703 ,        nan,        nan, ..., 18.030001 ,        nan,\n",
       "                nan],\n",
       "        [ 5.405406 ,        nan,        nan, ..., 18.51     ,        nan,\n",
       "                nan],\n",
       "        ...,\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan],\n",
       "        [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "                nan]]),\n",
       " 'NPMS': array([[2985.074   ,         nan,         nan, ...,   20.690001,\n",
       "                 nan,         nan],\n",
       "        [2957.937   ,         nan,         nan, ...,   20.610001,\n",
       "                 nan,         nan],\n",
       "        [2887.4031  ,         nan,         nan, ...,   20.610001,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan]]),\n",
       " 'SPVF': array([[2807.5       ,           nan,           nan, ...,   21.0414121 ,\n",
       "                   nan,           nan],\n",
       "        [2856.25      ,           nan,           nan, ...,   21.0016332 ,\n",
       "                   nan,           nan],\n",
       "        [2941.5       ,           nan,           nan, ...,   20.97244137,\n",
       "                   nan,           nan],\n",
       "        ...,\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan],\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan],\n",
       "        [          nan,           nan,           nan, ...,           nan,\n",
       "                   nan,           nan]]),\n",
       " 'TMMS': array([[5043.2441  ,         nan,         nan, ...,   20.110001,\n",
       "                 nan,         nan],\n",
       "        [5113.5142  ,         nan,         nan, ...,   20.09    ,\n",
       "                 nan,         nan],\n",
       "        [4994.5952  ,         nan,         nan, ...,   20.059999,\n",
       "                 nan,         nan],\n",
       "        ...,\n",
       "        [4066.6121  ,         nan,         nan, ...,   16.629999,\n",
       "                 nan,         nan],\n",
       "        [4009.4121  ,         nan,         nan, ...,   16.629999,\n",
       "                 nan,         nan],\n",
       "        [3888.4971  ,         nan,         nan, ...,   16.629999,\n",
       "                 nan,         nan]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1=pd.DataFrame(data, index=['A'])\n",
    "data\n",
    "#cur_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=np.vstack(( data['R1C1'], data['R1H1'], data['R1P1'],  data['TMMS'], data['LRMS'], data['NPMS'], data['SPVF'], data['BGZOB1'], data['BGZOB2'], data['BGZOB3'], data['BGZOB4'], data['Green1'], data['Green2'], data['Green3'], data['MC1'], data['MC2'], data['MC3'], data['MC4'], data['MC5'], data['MC6']))\n",
    "#\n",
    "df=pd.DataFrame(data=merged, columns=['CO2', 'column 2', 'column 3', 'column 4', 'column 5', 'column 6', 'column 7', 'column 8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO2</th>\n",
       "      <th>column 2</th>\n",
       "      <th>column 3</th>\n",
       "      <th>column 4</th>\n",
       "      <th>column 5</th>\n",
       "      <th>column 6</th>\n",
       "      <th>column 7</th>\n",
       "      <th>column 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7495.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.99000</td>\n",
       "      <td>18.988224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7715.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.99000</td>\n",
       "      <td>18.761198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7910.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.99000</td>\n",
       "      <td>18.761198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7875.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.99000</td>\n",
       "      <td>18.761198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8325.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.99000</td>\n",
       "      <td>18.587071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691424</th>\n",
       "      <td>593.66775</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.56575</td>\n",
       "      <td>13.794750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691425</th>\n",
       "      <td>594.45275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.53375</td>\n",
       "      <td>13.797750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691426</th>\n",
       "      <td>596.09300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54150</td>\n",
       "      <td>13.797000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691427</th>\n",
       "      <td>599.28900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.54425</td>\n",
       "      <td>13.798250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691428</th>\n",
       "      <td>595.37475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.51450</td>\n",
       "      <td>13.805000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483919 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                CO2  column 2  column 3  column 4  column 5   column 6  \\\n",
       "0        7495.00000       NaN       NaN       NaN  15.99000  18.988224   \n",
       "1        7715.00000       0.0       NaN       NaN  15.99000  18.761198   \n",
       "2        7910.00000       0.0       NaN       NaN  15.99000  18.761198   \n",
       "3        7875.00000       0.0       NaN       NaN  15.99000  18.761198   \n",
       "4        8325.00000       0.0       NaN       NaN  15.99000  18.587071   \n",
       "...             ...       ...       ...       ...       ...        ...   \n",
       "2691424   593.66775       NaN  0.100000      0.01   0.56575  13.794750   \n",
       "2691425   594.45275       NaN  0.100000      0.01   0.53375  13.797750   \n",
       "2691426   596.09300       NaN  0.100000      0.01   0.54150  13.797000   \n",
       "2691427   599.28900       NaN  0.100000      0.01   0.54425  13.798250   \n",
       "2691428   595.37475       NaN  0.099667      0.01   0.51450  13.805000   \n",
       "\n",
       "         column 7  column 8  \n",
       "0             NaN       NaN  \n",
       "1             NaN       NaN  \n",
       "2             NaN       NaN  \n",
       "3             NaN       NaN  \n",
       "4             NaN       NaN  \n",
       "...           ...       ...  \n",
       "2691424       NaN   416.725  \n",
       "2691425       NaN   417.225  \n",
       "2691426       NaN   418.325  \n",
       "2691427       NaN   418.850  \n",
       "2691428       NaN   418.850  \n",
       "\n",
       "[1483919 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=['CO2'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we'll need to make sure all the data is in the same units, across all sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the goal is to build up a numpy array where each row is an observation (an individual site/depth/time; e.g., Calhoun R1C1 on January 1st at 12:15 pm) and each column is a parameter (e.g., CO2, Soil moisture, O2, etc). We'll have lots of NaN values, and that's fine for now.\n",
    "\n",
    "What I would recommend doing is building up one site/pit at a time, then combining arrays later on. For example, the R1C1 array should look like:\n",
    "\n",
    "|     |CO2|precip|SoilMoisture|BulkEC|Temp|O2|\n",
    "|---|---|---|---|---|---|---|\n",
    "|12/12/20 12:15 pm @ 25 cm | 5000 ppm | 0 | 0.24 | NaN | 4.37 | 19.1 |\n",
    "|12/12/20 12:30 pm @ 25 cm | 5169 ppm | 0 | 0.26 | NaN | 4.45 | 19.2 |\n",
    "|12/12/20 12:45 pm @ 25 cm | 5120 ppm | 0 | 0.29 | NaN | 4.42 | 19.1 |\n",
    "|12/12/20 01:00 pm @ 25 cm | 5148 ppm | 0 | 0.26 | NaN | 4.49 | 19.2 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "|07/04/18 09:15 am @ 150 cm | 6952 ppm | 0.01 | 0.39 | NaN | 4.3 | 19.1 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| etc | etc | etc | etc | etc | etc | etc |\n",
    "\n",
    "Except without the columns or index labels. You could also set it up as a pandas dataframe (ie, with column and index labels) then extract the values later on. Whichever is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_array=df.to_numpy()\n",
    "output=final_array.copy()\n",
    "outfile = '../../merged_processed_data/MergeProcessedData.csv'\n",
    "#output.to_csv(outfile, na_rep=-99999, index_label='record', float_format='%.3f')\n",
    "np.savetxt(outfile, output, fmt='%.4f', delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.49500000e+03,            nan,            nan, ...,\n",
       "        1.89882244e+01,            nan,            nan],\n",
       "       [7.71500000e+03, 0.00000000e+00,            nan, ...,\n",
       "        1.87611981e+01,            nan,            nan],\n",
       "       [7.91000000e+03, 0.00000000e+00,            nan, ...,\n",
       "        1.87611981e+01,            nan,            nan],\n",
       "       ...,\n",
       "       [5.96093000e+02,            nan, 1.00000000e-01, ...,\n",
       "        1.37970000e+01,            nan, 4.18325000e+02],\n",
       "       [5.99289000e+02,            nan, 1.00000000e-01, ...,\n",
       "        1.37982500e+01,            nan, 4.18850000e+02],\n",
       "       [5.95374750e+02,            nan, 9.96666667e-02, ...,\n",
       "        1.38050000e+01,            nan, 4.18850000e+02]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
