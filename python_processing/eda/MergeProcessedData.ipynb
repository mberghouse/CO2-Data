{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# sites is a list of locations (e.g., \"Calhoun\"),\n",
    "# all_pits is a dict with sites as keys, and lists of pits \n",
    "# at each site as the values\n",
    "all_pits = {}\n",
    "sites = os.listdir('../../processed_data')\n",
    "\n",
    "# Loop through all sites to construct dict of sites/pits\n",
    "for site in sites:\n",
    "    cur_pits = []\n",
    "    for pro_file in os.listdir(os.path.join('../../processed_data/', site)):\n",
    "        cur_pits.append(pro_file.split('_')[0])\n",
    "        \n",
    "    all_pits[site] = cur_pits\n",
    "\n",
    "# Data will be a dict with pits as keys, np arrays as values\n",
    "data = {}\n",
    "# This is a list of all the features for R1C1. We'll want to make this an exhaustive list\n",
    "# of all potential features across all pits\n",
    "features = ['CO2', 'precip', 'SoilMoisture', 'BulkEC', 'Temp', 'O2', 'WaterPotential', 'ReductionPotential']\n",
    "m = len(features)\n",
    "\n",
    "# Load in and merge all files\n",
    "# NOTE: Only doing Calhoun R1C1 for now, but you get the idea\n",
    "i = 0\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        \n",
    "        if pit == 'R1C1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'R1H1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "    \n",
    "        if pit == 'R1P1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'LRMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'TMMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'NPMS':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'SPVF':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "          \n",
    "      \n",
    "        if pit == 'BGZOB1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "       \n",
    "        if pit == 'BGZOB3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'BGZOB4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'Green2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'Green3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC2':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC3':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC4':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "        if pit == 'MC5':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            \n",
    "        if pit == 'MC6':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1C1\n",
      "R1C2\n",
      "R1H1\n",
      "R1P1\n",
      "BGZOB1\n",
      "BGZOB2\n",
      "BGZOB3\n",
      "BGZOB4\n",
      "Green1\n",
      "Green2\n",
      "Green3\n",
      "Green\n",
      "MC1\n",
      "MC2\n",
      "MC3\n",
      "MC4\n",
      "MC5\n",
      "MC6\n",
      "SFPit1\n",
      "LRMS\n",
      "NPMS\n",
      "SPMS\n",
      "SPVF\n",
      "TMMS\n",
      "['5', '10', '100', '60', '30']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ApogeeTemp_10cm.deg.C', 'DecagonMPS6Temp_10cm.deg.C']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=2\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        print (pit)\n",
    "print (unique_depths)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1C1': array([[ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.],\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.],\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.]]),\n",
       " 'R1H1': array([[0.0000e+00,        nan, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00,        nan, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00,        nan, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [3.3925e+04,        nan, 0.0000e+00, ..., 1.6040e+01, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.3920e+04,        nan, 0.0000e+00, ..., 1.6040e+01, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.3950e+04,        nan, 0.0000e+00, ..., 1.6040e+01, 0.0000e+00,\n",
       "         0.0000e+00]]),\n",
       " 'R1P1': array([[ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.],\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.],\n",
       "        [nan, nan,  0., ..., nan,  0.,  0.]]),\n",
       " 'BGZOB1': array([[ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        [ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        [ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        ...,\n",
       "        [ 2.11123275e+03,             nan,  1.34000000e-01, ...,\n",
       "          1.96300000e+01, -3.35075000e+03,  4.86675000e+02],\n",
       "        [ 2.11038875e+03,             nan,  1.33750000e-01, ...,\n",
       "          1.96300000e+01, -3.38275000e+03,  4.86825000e+02],\n",
       "        [ 2.11038875e+03,             nan,  1.33750000e-01, ...,\n",
       "          1.96202500e+01, -3.34425000e+03,  4.86900000e+02]]),\n",
       " 'BGZOB2': array([[ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        [ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        [ 0.00000000e+00,             nan,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,             nan],\n",
       "        ...,\n",
       "        [ 1.58670275e+03,             nan,  7.60000000e-02, ...,\n",
       "          1.94555000e+01, -2.22450000e+03,  6.81800000e+02],\n",
       "        [ 1.58636225e+03,             nan,  7.60000000e-02, ...,\n",
       "          1.94647500e+01, -3.63900000e+03,  6.81850000e+02],\n",
       "        [ 1.57774450e+03,             nan,  7.60000000e-02, ...,\n",
       "          1.94750000e+01, -2.93575000e+03,  6.82050000e+02]]),\n",
       " 'BGZOB3': array([[ 0.000000e+00,           nan,  0.000000e+00, ...,  0.000000e+00,\n",
       "          0.000000e+00,           nan],\n",
       "        [ 0.000000e+00,           nan,  0.000000e+00, ...,  0.000000e+00,\n",
       "          0.000000e+00,           nan],\n",
       "        [ 0.000000e+00,           nan,  0.000000e+00, ...,  0.000000e+00,\n",
       "          0.000000e+00,           nan],\n",
       "        ...,\n",
       "        [ 3.435928e+03,           nan,  5.200000e-02, ...,  1.999250e+01,\n",
       "         -4.609000e+02,  5.697000e+02],\n",
       "        [ 3.446174e+03,           nan,  5.200000e-02, ...,  1.999225e+01,\n",
       "         -4.303250e+02,  5.697000e+02],\n",
       "        [ 3.469785e+03,           nan,  5.200000e-02, ...,  1.999450e+01,\n",
       "         -4.573250e+02,  5.697000e+02]]),\n",
       " 'BGZOB4': array([[ 1.13009875e+03,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [ 1.25472175e+03,             nan,             nan, ...,\n",
       "                     nan,             nan,             nan],\n",
       "        [ 1.23952775e+03,             nan,  8.45000000e-02, ...,\n",
       "                     nan, -1.15250000e+01,             nan],\n",
       "        ...,\n",
       "        [ 1.28139775e+03,             nan,  1.16000000e-01, ...,\n",
       "          1.84840000e+01, -1.67325000e+03,  6.09625000e+02],\n",
       "        [ 1.28035025e+03,             nan,  1.16000000e-01, ...,\n",
       "          1.85087500e+01, -1.65175000e+03,  6.09700000e+02],\n",
       "        [ 1.28324200e+03,             nan,  1.16000000e-01, ...,\n",
       "          1.85105000e+01, -1.64950000e+03,  6.09550000e+02]]),\n",
       " 'Green1': array([[ 8.84367000e+02,             nan,  2.57500000e-01, ...,\n",
       "                     nan, -1.29000000e+01,  0.00000000e+00],\n",
       "        [ 9.27735750e+02,             nan,  2.57750000e-01, ...,\n",
       "                     nan, -1.35500000e+01,  0.00000000e+00],\n",
       "        [ 9.97565000e+02,             nan,  2.58000000e-01, ...,\n",
       "                     nan, -1.34250000e+01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 3.35936700e+03,             nan,  1.43000000e-01, ...,\n",
       "          2.00022500e+01, -1.65750000e+01,  0.00000000e+00],\n",
       "        [ 3.37078400e+03,             nan,  1.42250000e-01, ...,\n",
       "          1.99905000e+01, -1.65500000e+01,  0.00000000e+00],\n",
       "        [ 3.38474125e+03,             nan,  1.42750000e-01, ...,\n",
       "          1.99795000e+01, -1.65750000e+01,  0.00000000e+00]]),\n",
       " 'Green2': array([[ 5.58145636e+02,             nan,  1.69818182e-01, ...,\n",
       "          2.08950000e+01, -9.57272727e+00,  0.00000000e+00],\n",
       "        [ 1.67497663e+03,             nan,  1.70625000e-01, ...,\n",
       "          2.11452500e+01, -9.60625000e+00,  0.00000000e+00],\n",
       "        [ 1.87421306e+03,             nan,  1.71000000e-01, ...,\n",
       "          2.11975625e+01, -9.33125000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.31705525e+03,             nan,  1.46000000e-01, ...,\n",
       "          1.91380625e+01, -1.84500000e+01,  0.00000000e+00],\n",
       "        [ 2.32309125e+03,             nan,  1.46000000e-01, ...,\n",
       "          1.91512500e+01, -1.84625000e+01,  0.00000000e+00],\n",
       "        [ 2.32879694e+03,             nan,  1.46000000e-01, ...,\n",
       "          1.91502500e+01, -1.84625000e+01,  0.00000000e+00]]),\n",
       " 'Green3': array([[ 2.03324100e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.02070000e+01, -1.04000000e+01,  0.00000000e+00],\n",
       "        [ 2.03914300e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.11652500e+01, -1.02750000e+01,  0.00000000e+00],\n",
       "        [ 2.05904400e+03,             nan,  1.75000000e-01, ...,\n",
       "          2.12462500e+01, -1.03000000e+01,  0.00000000e+00],\n",
       "        ...,\n",
       "        [ 2.98100700e+03,             nan,  9.97500000e-02, ...,\n",
       "          1.92872500e+01, -1.51500000e+01,  0.00000000e+00],\n",
       "        [ 2.97676050e+03,             nan,  9.92500000e-02, ...,\n",
       "          1.92837500e+01, -1.49500000e+01,  0.00000000e+00],\n",
       "        [ 2.97615025e+03,             nan,  9.92500000e-02, ...,\n",
       "          1.92830000e+01, -1.53000000e+01,  0.00000000e+00]]),\n",
       " 'MC1': array([[ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC2': array([[ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        [ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        [ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        ...,\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0., nan,  0., ...,  0.,  0.,  0.]]),\n",
       " 'MC3': array([[ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan],\n",
       "        [nan, nan, nan, ..., nan,  0., nan]]),\n",
       " 'MC4': array([[ 0., nan,  0., ...,  0., nan,  0.],\n",
       "        [ 0., nan,  0., ...,  0., nan,  0.],\n",
       "        [ 0., nan,  0., ...,  0., nan,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC5': array([[ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        [ 0., nan, nan, ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'MC6': array([[ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        [ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        [ 0., nan,  0., ...,  0.,  0., nan],\n",
       "        ...,\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan],\n",
       "        [nan, nan,  0., ..., nan,  0., nan]]),\n",
       " 'LRMS': array([[4835.1362  ,         nan,    0.      , ...,   17.559999,\n",
       "            0.      ,    0.      ],\n",
       "        [4846.6221  ,         nan,    0.      , ...,   17.57    ,\n",
       "            0.      ,    0.      ],\n",
       "        [4867.5679  ,         nan,    0.      , ...,   17.57    ,\n",
       "            0.      ,    0.      ],\n",
       "        ...,\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ]]),\n",
       " 'NPMS': array([[2985.074   ,         nan,    0.      , ...,   20.690001,\n",
       "            0.      ,    0.      ],\n",
       "        [2957.937   ,         nan,    0.      , ...,   20.610001,\n",
       "            0.      ,    0.      ],\n",
       "        [2887.4031  ,         nan,    0.      , ...,   20.610001,\n",
       "            0.      ,    0.      ],\n",
       "        ...,\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ],\n",
       "        [        nan,         nan,    0.      , ...,         nan,\n",
       "            0.      ,    0.      ]]),\n",
       " 'SPVF': array([[2.80750000e+003,             nan,             nan, ...,\n",
       "         2.10414121e+001,             nan,             nan],\n",
       "        [2.85625000e+003,             nan,             nan, ...,\n",
       "         2.10016332e+001, 5.24000000e+000, 5.24000000e+000],\n",
       "        [2.94150000e+003,             nan, 5.24000000e+000, ...,\n",
       "         2.09724414e+001, 5.24000000e+000, 5.24000000e+000],\n",
       "        ...,\n",
       "        [            nan,             nan, 1.00584727e-311, ...,\n",
       "                     nan, 1.00584727e-311, 1.00584727e-311],\n",
       "        [            nan,             nan, 1.00584727e-311, ...,\n",
       "                     nan, 1.00584727e-311, 1.00584727e-311],\n",
       "        [            nan,             nan, 1.00584727e-311, ...,\n",
       "                     nan, 0.00000000e+000, 0.00000000e+000]]),\n",
       " 'TMMS': array([[5043.2441  ,         nan,    0.      , ...,   20.110001,\n",
       "            0.      ,    0.      ],\n",
       "        [5113.5142  ,         nan,    0.      , ...,   20.09    ,\n",
       "            0.      ,    0.      ],\n",
       "        [4994.5952  ,         nan,    0.      , ...,   20.059999,\n",
       "            0.      ,    0.      ],\n",
       "        ...,\n",
       "        [4066.6121  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ],\n",
       "        [4009.4121  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ],\n",
       "        [3888.4971  ,         nan,    0.      , ...,   16.629999,\n",
       "            0.      ,    0.      ]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1=pd.DataFrame(data, index=['A'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we'll need to make sure all the data is in the same units, across all sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the goal is to build up a numpy array where each row is an observation (an individual site/depth/time; e.g., Calhoun R1C1 on January 1st at 12:15 pm) and each column is a parameter (e.g., CO2, Soil moisture, O2, etc). We'll have lots of NaN values, and that's fine for now.\n",
    "\n",
    "What I would recommend doing is building up one site/pit at a time, then combining arrays later on. For example, the R1C1 array should look like:\n",
    "\n",
    "|     |CO2|precip|SoilMoisture|BulkEC|Temp|O2|\n",
    "|---|---|---|---|---|---|---|\n",
    "|12/12/20 12:15 pm @ 25 cm | 5000 ppm | 0 | 0.24 | NaN | 4.37 | 19.1 |\n",
    "|12/12/20 12:30 pm @ 25 cm | 5169 ppm | 0 | 0.26 | NaN | 4.45 | 19.2 |\n",
    "|12/12/20 12:45 pm @ 25 cm | 5120 ppm | 0 | 0.29 | NaN | 4.42 | 19.1 |\n",
    "|12/12/20 01:00 pm @ 25 cm | 5148 ppm | 0 | 0.26 | NaN | 4.49 | 19.2 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "|07/04/18 09:15 am @ 150 cm | 6952 ppm | 0.01 | 0.39 | NaN | 4.3 | 19.1 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| etc | etc | etc | etc | etc | etc | etc |\n",
    "\n",
    "Except without the columns or index labels. You could also set it up as a pandas dataframe (ie, with column and index labels) then extract the values later on. Whichever is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-cb49a5ed17eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../../processed_data/MergeProcessedData.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_rep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m99999\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DateTime.MST'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "output=cur_data.copy()\n",
    "outfile = '../../processed_data/MergeProcessedData.csv'\n",
    "output.to_csv(outfile, na_rep=-99999, index_label='DateTime.MST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '../../processed_data/MergeProcessedData.csv'\n",
    "\n",
    "np.savetxt(outfile, output, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(output).to_csv('../../processed_data/MergeProcessedData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R1C1': array([[ 0., nan,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan]]),\n",
       " 'R1H1': array([[0.0000e+00,        nan, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00,        nan, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00,        nan, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [1.0030e+04,        nan, 2.1000e-01, 1.7000e-02, 2.2870e+01,\n",
       "         1.7470e+01],\n",
       "        [1.0025e+04,        nan, 2.1000e-01, 1.7000e-02, 2.2910e+01,\n",
       "         1.7470e+01],\n",
       "        [1.0110e+04,        nan, 2.1000e-01, 1.7000e-02, 2.2930e+01,\n",
       "         1.7470e+01]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
