{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# sites is a list of locations (e.g., \"Calhoun\"),\n",
    "# all_pits is a dict with sites as keys, and lists of pits \n",
    "# at each site as the values\n",
    "all_pits = {}\n",
    "sites = os.listdir('../../processed_data')\n",
    "\n",
    "# Loop through all sites to construct dict of sites/pits\n",
    "for site in sites:\n",
    "    cur_pits = []\n",
    "    for pro_file in os.listdir(os.path.join('../../processed_data/', site)):\n",
    "        cur_pits.append(pro_file.split('_')[0])\n",
    "        \n",
    "    all_pits[site] = cur_pits\n",
    "\n",
    "# Data will be a dict with pits as keys, np arrays as values\n",
    "data = {}\n",
    "# This is a list of all the features for R1C1. We'll want to make this an exhaustive list\n",
    "# of all potential features across all pits\n",
    "features = ['CO2', 'precip', 'SoilMoisture', 'BulkEC', 'Temp', 'O2']\n",
    "m = len(features)\n",
    "\n",
    "# Load in and merge all files\n",
    "# NOTE: Only doing Calhoun R1C1 for now, but you get the idea\n",
    "i = 0\n",
    "for site in all_pits.keys():\n",
    "    for pit in all_pits[site]:\n",
    "        \n",
    "        if pit == 'R1C1':\n",
    "            infile = os.path.join('../../processed_data', site, '%s_processed.csv' % pit)\n",
    "            df = pd.read_csv(infile, parse_dates=[0], index_col=0, na_values=[-99999])\n",
    "            depths = [col.split('_')[1].split('cm')[0] for col in df.columns]\n",
    "            depths = [d for d in depths if d != 'precip.mm']\n",
    "            unique_depths = list(set(depths))\n",
    "            \n",
    "            # t is the number of time steps\n",
    "            t = df.shape[0]\n",
    "            \n",
    "            # data shape is the # of observations (# time points * depths), # features\n",
    "            cur_data = np.empty((t*len(unique_depths), m), dtype=float)\n",
    "            \n",
    "            for i, depth in enumerate(unique_depths):\n",
    "                depth_cols = [col for col in df.columns if '_%scm' % depth in col]\n",
    "                for j, feature in enumerate(features):\n",
    "                    # Look for columns with this feature and depth in the name\n",
    "                    if feature == 'precip':\n",
    "                        col = 'PRISM_precip.mm'\n",
    "                    else:\n",
    "                        matches = [col for col in depth_cols if feature in col]\n",
    "                        \n",
    "                        # Correct for fact that \"O2\" search returns \"CO2\" as well\n",
    "                        if feature == 'O2':\n",
    "                            matches = [col for col in matches if 'CO2' not in col]\n",
    "                        \n",
    "                        if len(matches) > 1:\n",
    "                            raise ValueError(\"\"\"More than one possible match found for %s %s %scm %s\"\"\" %(site, pit, depth, feature))\n",
    "                        elif len(matches) == 1:\n",
    "                            col = matches[0]\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "                    # Finally, now that we have the column name\n",
    "                    cur_data[i*t:(i+1)*t, j] = df[col].values\n",
    "                    \n",
    "            data[pit] = cur_data.copy()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we'll need to make sure all the data is in the same units, across all sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the goal is to build up a numpy array where each row is an observation (an individual site/depth/time; e.g., Calhoun R1C1 on January 1st at 12:15 pm) and each column is a parameter (e.g., CO2, Soil moisture, O2, etc). We'll have lots of NaN values, and that's fine for now.\n",
    "\n",
    "What I would recommend doing is building up one site/pit at a time, then combining arrays later on. For example, the R1C1 array should look like:\n",
    "\n",
    "|     |CO2|precip|SoilMoisture|BulkEC|Temp|O2|\n",
    "|---|---|---|---|---|---|---|\n",
    "|12/12/20 12:15 pm @ 25 cm | 5000 ppm | 0 | 0.24 | NaN | 4.37 | 19.1 |\n",
    "|12/12/20 12:30 pm @ 25 cm | 5169 ppm | 0 | 0.26 | NaN | 4.45 | 19.2 |\n",
    "|12/12/20 12:45 pm @ 25 cm | 5120 ppm | 0 | 0.29 | NaN | 4.42 | 19.1 |\n",
    "|12/12/20 01:00 pm @ 25 cm | 5148 ppm | 0 | 0.26 | NaN | 4.49 | 19.2 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "|07/04/18 09:15 am @ 150 cm | 6952 ppm | 0.01 | 0.39 | NaN | 4.3 | 19.1 |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| etc | etc | etc | etc | etc | etc | etc |\n",
    "\n",
    "Except without the columns or index labels. You could also set it up as a pandas dataframe (ie, with column and index labels) then extract the values later on. Whichever is easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
